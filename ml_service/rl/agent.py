"""PPO Agent for F1 Aerodynamic ControlProximal Policy Optimization for DRS and flap control"""import torchimport torch.nn as nnimport torch.optim as optimimport numpy as npfrom typing import Dict, Any, List, Tuple, Optionalfrom collections import dequeimport timefrom .environment import F1AeroControlEnvclass ActorCritic(nn.Module):    """    Actor-Critic network for PPO        Actor: Policy network (state -> action distribution)    Critic: Value network (state -> state value)    """        def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):        super(ActorCritic, self).__init__()                # Shared feature extractor        self.shared = nn.Sequential(            nn.Linear(state_dim, hidden_dim),            nn.ReLU(),            nn.Linear(hidden_dim, hidden_dim),            nn.ReLU()        )                # Actor head (policy)        self.actor_mean = nn.Sequential(            nn.Linear(hidden_dim, hidden_dim // 2),            nn.ReLU(),            nn.Linear(hidden_dim // 2, action_dim),            nn.Tanh()  # Actions in [-1, 1]        )                self.actor_logstd = nn.Parameter(torch.zeros(action_dim))                # Critic head (value function)        self.critic = nn.Sequential(            nn.Linear(hidden_dim, hidden_dim // 2),            nn.ReLU(),            nn.Linear(hidden_dim // 2, 1)        )        def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:        """        Forward pass                Returns:            (action_mean, state_value)        """        features = self.shared(state)        action_mean = self.actor_mean(features)        state_value = self.critic(features)                return action_mean, state_value        def get_action(self, state: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:        """        Sample action from policy                Returns:            (action, log_prob, value)        """        action_mean, value = self.forward(state)                if deterministic:            return action_mean, torch.zeros_like(action_mean), value                # Sample from Gaussian distribution        action_std = torch.exp(self.actor_logstd)        dist = torch.distributions.Normal(action_mean, action_std)        action = dist.sample()        log_prob = dist.log_prob(action).sum(dim=-1)                return action, log_prob, valueclass PPOAgent:    """    Proximal Policy Optimization Agent        Optimizes F1 aerodynamic control policy for lap time minimization    """        def __init__(        self,        state_dim: int = 10,        action_dim: int = 3,        hidden_dim: int = 256,        lr: float = 3e-4,        gamma: float = 0.99,        gae_lambda: float = 0.95,        clip_epsilon: float = 0.2,        value_coef: float = 0.5,        entropy_coef: float = 0.01,        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'    ):        self.device = torch.device(device)                # Hyperparameters        self.gamma = gamma        self.gae_lambda = gae_lambda        self.clip_epsilon = clip_epsilon        self.value_coef = value_coef        self.entropy_coef = entropy_coef                # Networks        self.policy = ActorCritic(state_dim, action_dim, hidden_dim).to(self.device)        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)                # Training stats        self.training_stats = {            'episode_rewards': [],            'episode_lengths': [],            'lap_times': [],            'policy_losses': [],            'value_losses': [],            'total_losses': []        }                print(f"PPO Agent initialized on {device}")        print(f"  State dim: {state_dim}")        print(f"  Action dim: {action_dim}")        print(f"  Hidden dim: {hidden_dim}")        def select_action(self, state: np.ndarray, deterministic: bool = False) -> Tuple[np.ndarray, float, float]:        """        Select action using current policy                Returns:            (action, log_prob, value)        """        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)                with torch.no_grad():            action, log_prob, value = self.policy.get_action(state_tensor, deterministic)                return action.cpu().numpy()[0], log_prob.cpu().item(), value.cpu().item()        def compute_gae(        self,        rewards: List[float],        values: List[float],        dones: List[bool]    ) -> Tuple[np.ndarray, np.ndarray]:        """        Compute Generalized Advantage Estimation (GAE)                Returns:            (advantages, returns)        """        advantages = []        gae = 0                for t in reversed(range(len(rewards))):            if t == len(rewards) - 1:                next_value = 0            else:                next_value = values[t + 1]                        delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae            advantages.insert(0, gae)                advantages = np.array(advantages)        returns = advantages + np.array(values)                return advantages, returns        def update(        self,        states: np.ndarray,        actions: np.ndarray,        old_log_probs: np.ndarray,        advantages: np.ndarray,        returns: np.ndarray,        epochs: int = 10,        batch_size: int = 64    ) -> Dict[str, float]:        """        Update policy using PPO                Returns:            Training metrics        """        # Convert to tensors        states = torch.FloatTensor(states).to(self.device)        actions = torch.FloatTensor(actions).to(self.device)        old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)        advantages = torch.FloatTensor(advantages).to(self.device)        returns = torch.FloatTensor(returns).to(self.device)                # Normalize advantages        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)                # Training metrics        policy_losses = []        value_losses = []        total_losses = []                # PPO update        for epoch in range(epochs):            # Mini-batch updates            indices = np.arange(len(states))            np.random.shuffle(indices)                        for start in range(0, len(states), batch_size):                end = start + batch_size                batch_indices = indices[start:end]                                # Get current policy outputs                action_mean, values = self.policy(states[batch_indices])                                # Compute action log probabilities                action_std = torch.exp(self.policy.actor_logstd)                dist = torch.distributions.Normal(action_mean, action_std)                log_probs = dist.log_prob(actions[batch_indices]).sum(dim=-1)                entropy = dist.entropy().sum(dim=-1).mean()                                # PPO policy loss                ratio = torch.exp(log_probs - old_log_probs[batch_indices])                surr1 = ratio * advantages[batch_indices]                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages[batch_indices]                policy_loss = -torch.min(surr1, surr2).mean()                                # Value loss                value_loss = nn.MSELoss()(values.squeeze(), returns[batch_indices])                                # Total loss                loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy                                # Optimize                self.optimizer.zero_grad()                loss.backward()                nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)                self.optimizer.step()                                # Record losses                policy_losses.append(policy_loss.item())                value_losses.append(value_loss.item())                total_losses.append(loss.item())                return {            'policy_loss': np.mean(policy_losses),            'value_loss': np.mean(value_losses),            'total_loss': np.mean(total_losses)        }        def train(        self,        env: F1AeroControlEnv,        num_episodes: int = 1000,        max_steps: int = 1000,        update_interval: int = 2048,        save_interval: int = 100,        save_path: str = 'models/ppo_aero_control.pt'    ):        """        Train PPO agent                Args:            env: F1 aerodynamic control environment            num_episodes: Number of training episodes            max_steps: Maximum steps per episode            update_interval: Steps between policy updates            save_interval: Episodes between model saves            save_path: Path to save model checkpoints        """        print(f"\nStarting PPO training for {num_episodes} episodes...")                # Rollout buffer        states_buffer = []        actions_buffer = []        log_probs_buffer = []        rewards_buffer = []        values_buffer = []        dones_buffer = []                episode = 0        total_steps = 0                while episode < num_episodes:            state = env.reset()            episode_reward = 0            episode_length = 0                        for step in range(max_steps):                # Select action                action, log_prob, value = self.select_action(state)                                # Environment step                next_state, reward, done, info = env.step(action)                                # Store transition                states_buffer.append(state)                actions_buffer.append(action)                log_probs_buffer.append(log_prob)                rewards_buffer.append(reward)                values_buffer.append(value)                dones_buffer.append(done)                                episode_reward += reward                episode_length += 1                total_steps += 1                                state = next_state                                # Update policy                if total_steps % update_interval == 0:                    # Compute advantages                    advantages, returns = self.compute_gae(                        rewards_buffer, values_buffer, dones_buffer                    )                                        # Update                    metrics = self.update(                        np.array(states_buffer),                        np.array(actions_buffer),                        np.array(log_probs_buffer),                        advantages,                        returns                    )                                        # Record metrics                    self.training_stats['policy_losses'].append(metrics['policy_loss'])                    self.training_stats['value_losses'].append(metrics['value_loss'])                    self.training_stats['total_losses'].append(metrics['total_loss'])                                        # Clear buffer                    states_buffer = []                    actions_buffer = []                    log_probs_buffer = []                    rewards_buffer = []                    values_buffer = []                    dones_buffer = []                                        print(f"Update at step {total_steps}: "                          f"Policy Loss: {metrics['policy_loss']:.4f}, "                          f"Value Loss: {metrics['value_loss']:.4f}")                                if done:                    break                        # Record episode stats            self.training_stats['episode_rewards'].append(episode_reward)            self.training_stats['episode_lengths'].append(episode_length)            self.training_stats['lap_times'].append(info['lap_time'])                        episode += 1                        # Print progress            if episode % 10 == 0:                avg_reward = np.mean(self.training_stats['episode_rewards'][-10:])                avg_lap_time = np.mean(self.training_stats['lap_times'][-10:])                print(f"Episode {episode}/{num_episodes}: "                      f"Avg Reward: {avg_reward:.2f}, "                      f"Avg Lap Time: {avg_lap_time:.2f}s")                        # Save model            if episode % save_interval == 0:                self.save(save_path)                print(f"Model saved to {save_path}")                print("\nTraining complete!")        return self.training_stats        def save(self, path: str):        """Save model checkpoint"""        torch.save({            'policy_state_dict': self.policy.state_dict(),            'optimizer_state_dict': self.optimizer.state_dict(),            'training_stats': self.training_stats        }, path)        def load(self, path: str):        """Load model checkpoint"""        checkpoint = torch.load(path, map_location=self.device)        self.policy.load_state_dict(checkpoint['policy_state_dict'])        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])        self.training_stats = checkpoint['training_stats']        print(f"Model loaded from {path}")if __name__ == "__main__":    # Test PPO agent    print("Testing PPO Agent\n")        # Create environment    env = F1AeroControlEnv()        # Create agent    agent = PPOAgent(        state_dim=10,        action_dim=3,        hidden_dim=256    )        # Test action selection    print("\nTesting action selection:")    state = env.reset()    action, log_prob, value = agent.select_action(state)    print(f"  State shape: {state.shape}")    print(f"  Action: {action}")    print(f"  Log prob: {log_prob:.4f}")    print(f"  Value: {value:.4f}")        # Quick training test (5 episodes)    print("\nQuick training test (5 episodes):")    stats = agent.train(env, num_episodes=5, update_interval=100)        print(f"\n  Episodes completed: {len(stats['episode_rewards'])}")    print(f"  Average reward: {np.mean(stats['episode_rewards']):.2f}")    print(f"  Average lap time: {np.mean(stats['lap_times']):.2f}s")        print("\nÔ£ô PPO agent test complete!")