"""AeroTransformer API EndpointsFastAPI service for training and inference"""from fastapi import FastAPI, HTTPException, BackgroundTasksfrom pydantic import BaseModelfrom typing import Dict, Any, List, Optionalimport numpy as npimport torchfrom datetime import datetimeimport osfrom .inference import AeroTransformerInferencefrom .train import AeroTransformerTrainerfrom .model import create_aero_transformerfrom .dataset import CFDDataset# Initialize FastAPI appapp = FastAPI(title="AeroTransformer API", version="1.0.0")# Global inference serviceinference_service: Optional[AeroTransformerInference] = None# Training statetraining_state = {    'is_training': False,    'current_epoch': 0,    'total_epochs': 0,    'train_loss': 0.0,    'val_loss': 0.0,    'start_time': None}# Request/Response Modelsclass PredictRequest(BaseModel):    geometry: List[List[List[List[float]]]]  # (C, D, H, W)    return_timing: bool = Trueclass PredictResponse(BaseModel):    pressure: List[List[List[float]]]  # (D, H, W)    velocity: List[List[List[List[float]]]]  # (3, D, H, W)    turbulence: List[List[List[List[float]]]]  # (3, D, H, W)    inference_time_ms: Optional[float]class TrainRequest(BaseModel):    model_size: str = 'base'    batch_size: int = 4    learning_rate: float = 1e-4    epochs: int = 100    dataset_path: str = 'data/cfd_dataset'class TrainStatusResponse(BaseModel):    is_training: bool    current_epoch: int    total_epochs: int    train_loss: float    val_loss: float    elapsed_time: Optional[float]class MetricsResponse(BaseModel):    epoch: int    train_loss: Dict[str, float]    val_loss: Dict[str, float]    learning_rate: floatclass BenchmarkResponse(BaseModel):    mean_ms: float    std_ms: float    min_ms: float    max_ms: float    median_ms: float    p95_ms: float    p99_ms: float    target_met: bool# API Endpoints@app.on_event("startup")async def startup_event():    """Load model on startup"""    global inference_service        model_path = os.getenv('AEROTRANSFORMER_MODEL_PATH', 'checkpoints/aerotransformer/best_model.pt')        if os.path.exists(model_path):        try:            inference_service = AeroTransformerInference(                model_path=model_path,                model_size='base',                device='cuda'            )            print("Ô£ô AeroTransformer inference service loaded")        except Exception as e:            print(f"Ô£ù Failed to load model: {e}")            inference_service = None    else:        print(f"ÔÜá´©Å  Model not found at {model_path}")        inference_service = None@app.get("/")async def root():    """API root"""    return {        "service": "AeroTransformer API",        "version": "1.0.0",        "status": "ready" if inference_service else "model_not_loaded"    }@app.post("/api/ml/aerotransformer/predict", response_model=PredictResponse)async def predict(request: PredictRequest):    """    Predict flow fields from geometry        Target: <50ms inference time    """    if inference_service is None:        raise HTTPException(status_code=503, detail="Model not loaded")        try:        # Convert to numpy array        geometry = np.array(request.geometry, dtype=np.float32)                # Predict        result = inference_service.predict(            geometry=geometry,            return_timing=request.return_timing        )                # Convert to lists for JSON serialization        response = PredictResponse(            pressure=result['pressure'].tolist(),            velocity=result['velocity'].tolist(),            turbulence=result['turbulence'].tolist(),            inference_time_ms=result.get('inference_time_ms')        )                return response            except Exception as e:        raise HTTPException(status_code=500, detail=str(e))@app.post("/api/ml/aerotransformer/predict-batch")async def predict_batch(geometries: List[List[List[List[List[float]]]]], batch_size: int = 4):    """    Batch prediction for multiple geometries    """    if inference_service is None:        raise HTTPException(status_code=503, detail="Model not loaded")        try:        # Convert to numpy array        geometries_np = np.array(geometries, dtype=np.float32)                # Predict        result = inference_service.predict_batch(            geometries=geometries_np,            batch_size=batch_size        )                return {            'num_samples': result['num_samples'],            'total_time_ms': result['total_time_ms'],            'avg_time_ms': result['avg_time_ms'],            'results': [                {                    'pressure': r['pressure'].tolist(),                    'velocity': r['velocity'].tolist(),                    'turbulence': r['turbulence'].tolist()                }                for r in result['results']            ]        }            except Exception as e:        raise HTTPException(status_code=500, detail=str(e))@app.get("/api/ml/aerotransformer/benchmark", response_model=BenchmarkResponse)async def benchmark(num_iterations: int = 100):    """    Benchmark inference performance        Target: <50ms on RTX 4090    """    if inference_service is None:        raise HTTPException(status_code=503, detail="Model not loaded")        try:        results = inference_service.benchmark(num_iterations=num_iterations)                return BenchmarkResponse(            **results,            target_met=results['mean_ms'] < 50.0        )            except Exception as e:        raise HTTPException(status_code=500, detail=str(e))@app.post("/api/ml/aerotransformer/train")async def train(request: TrainRequest, background_tasks: BackgroundTasks):    """    Start model training        Training runs in background    """    global training_state        if training_state['is_training']:        raise HTTPException(status_code=409, detail="Training already in progress")        # Start training in background    background_tasks.add_task(        _train_model,        request.model_size,        request.batch_size,        request.learning_rate,        request.epochs,        request.dataset_path    )        return {        "message": "Training started",        "config": request.dict()    }def _train_model(    model_size: str,    batch_size: int,    learning_rate: float,    epochs: int,    dataset_path: str):    """Background training task"""    global training_state        try:        training_state['is_training'] = True        training_state['total_epochs'] = epochs        training_state['start_time'] = datetime.now()                # Create config        config = {            'model_size': model_size,            'volume_size': (64, 64, 64),            'batch_size': batch_size,            'learning_rate': learning_rate,            'weight_decay': 0.01,            'epochs': epochs,            'num_workers': 4,            'grad_clip': 1.0,            'log_interval': 10,            'save_interval': 10,            'log_dir': 'runs/aerotransformer',            'checkpoint_dir': 'checkpoints/aerotransformer',            'dataset_path': dataset_path        }                # Create model        model = create_aero_transformer(            model_size=config['model_size'],            volume_size=config['volume_size']        )                # Create datasets        train_dataset = CFDDataset(            data_dir=config['dataset_path'],            split='train'        )        val_dataset = CFDDataset(            data_dir=config['dataset_path'],            split='val'        )                # Create trainer        trainer = AeroTransformerTrainer(            model=model,            train_dataset=train_dataset,            val_dataset=val_dataset,            config=config        )                # Train        trainer.train(num_epochs=epochs)                training_state['is_training'] = False            except Exception as e:        print(f"Training error: {e}")        training_state['is_training'] = False@app.get("/api/ml/aerotransformer/train-status", response_model=TrainStatusResponse)async def train_status():    """Get current training status"""    elapsed_time = None    if training_state['start_time']:        elapsed_time = (datetime.now() - training_state['start_time']).total_seconds()        return TrainStatusResponse(        is_training=training_state['is_training'],        current_epoch=training_state['current_epoch'],        total_epochs=training_state['total_epochs'],        train_loss=training_state['train_loss'],        val_loss=training_state['val_loss'],        elapsed_time=elapsed_time    )@app.get("/api/ml/aerotransformer/models")async def list_models():    """List available model checkpoints"""    checkpoint_dir = 'checkpoints/aerotransformer'        if not os.path.exists(checkpoint_dir):        return {"models": []}        models = []    for filename in os.listdir(checkpoint_dir):        if filename.endswith('.pt'):            filepath = os.path.join(checkpoint_dir, filename)            stat = os.stat(filepath)                        models.append({                'filename': filename,                'size_mb': stat.st_size / (1024 * 1024),                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()            })        return {"models": models}@app.post("/api/ml/aerotransformer/load-model")async def load_model(model_path: str):    """Load a specific model checkpoint"""    global inference_service        if not os.path.exists(model_path):        raise HTTPException(status_code=404, detail="Model not found")        try:        inference_service = AeroTransformerInference(            model_path=model_path,            model_size='base',            device='cuda'        )                return {"message": "Model loaded successfully", "model_path": model_path}            except Exception as e:        raise HTTPException(status_code=500, detail=str(e))if __name__ == "__main__":    import uvicorn    uvicorn.run(app, host="0.0.0.0", port=8003)