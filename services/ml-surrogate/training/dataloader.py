"""PyTorch DataLoader for F1 Aerodynamic DatasetLoads HDF5 data for ML surrogate training"""import torchfrom torch.utils.data import Dataset, DataLoaderimport h5pyimport numpy as npfrom pathlib import Pathfrom typing import Dict, Tuple, Optionalimport logginglogger = logging.getLogger(__name__)class F1AeroDataset(Dataset):    """    PyTorch Dataset for F1 aerodynamic data.        Loads data from HDF5 file generated by generate_dataset.py    """        def __init__(        self,        hdf5_file: str,        normalize: bool = True,        device: str = 'cpu'    ):        """        Initialize dataset.                Args:            hdf5_file: Path to HDF5 dataset file            normalize: Normalize inputs/outputs            device: Device to load tensors to        """        self.hdf5_file = Path(hdf5_file)        self.normalize = normalize        self.device = device                if not self.hdf5_file.exists():            raise FileNotFoundError(f"Dataset not found: {hdf5_file}")                # Load dataset        self._load_data()                # Compute normalization statistics        if self.normalize:            self._compute_normalization()                logger.info(f"Dataset loaded: {len(self)} samples from {hdf5_file}")        def _load_data(self):        """Load data from HDF5 file"""        with h5py.File(self.hdf5_file, 'r') as f:            # Load parameters (inputs)            self.parameters = {}            for key in f['parameters'].keys():                self.parameters[key] = f['parameters'][key][:]                        # Load results (outputs)            self.results = {}            for key in f['results'].keys():                self.results[key] = f['results'][key][:]                        # Load pressure distribution            self.pressure = f['pressure_distribution']['cp'][:]                        # Load metadata            self.n_samples = f['metadata'].attrs['n_samples']            self.n_panels = f['metadata'].attrs['n_panels']        def _compute_normalization(self):        """Compute normalization statistics"""        # Input normalization (parameters)        self.param_mean = {}        self.param_std = {}                for key, values in self.parameters.items():            self.param_mean[key] = float(np.mean(values))            self.param_std[key] = float(np.std(values))                # Output normalization (results)        self.result_mean = {}        self.result_std = {}                for key, values in self.results.items():            self.result_mean[key] = float(np.mean(values))            self.result_std[key] = float(np.std(values))                # Pressure normalization        self.pressure_mean = float(np.mean(self.pressure))        self.pressure_std = float(np.std(self.pressure))                logger.debug("Normalization statistics computed")        def __len__(self) -> int:        """Return dataset size"""        return self.n_samples        def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:        """        Get single sample.                Args:            idx: Sample index                    Returns:            (inputs, outputs) tuple        """        # Prepare inputs (design parameters)        inputs = []        for key in sorted(self.parameters.keys()):            value = self.parameters[key][idx]                        if self.normalize:                value = (value - self.param_mean[key]) / (self.param_std[key] + 1e-8)                        inputs.append(value)                inputs = torch.tensor(inputs, dtype=torch.float32, device=self.device)                # Prepare outputs        outputs = {}                # Force coefficients        for key in ['cl', 'cd', 'cm']:            value = self.results[key][idx]                        if self.normalize:                value = (value - self.result_mean[key]) / (self.result_std[key] + 1e-8)                        outputs[key] = torch.tensor(value, dtype=torch.float32, device=self.device)                # Pressure distribution        pressure = self.pressure[idx]                if self.normalize:            pressure = (pressure - self.pressure_mean) / (self.pressure_std + 1e-8)                outputs['pressure'] = torch.tensor(pressure, dtype=torch.float32, device=self.device)                return inputs, outputs        def denormalize_outputs(self, outputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:        """        Denormalize model outputs.                Args:            outputs: Normalized outputs                    Returns:            Denormalized outputs        """        if not self.normalize:            return outputs                denorm = {}                for key in ['cl', 'cd', 'cm']:            if key in outputs:                denorm[key] = outputs[key] * self.result_std[key] + self.result_mean[key]                if 'pressure' in outputs:            denorm['pressure'] = outputs['pressure'] * self.pressure_std + self.pressure_mean                return denorm        def get_input_dim(self) -> int:        """Get input dimension"""        return len(self.parameters)        def get_output_dims(self) -> Dict[str, int]:        """Get output dimensions"""        return {            'cl': 1,            'cd': 1,            'cm': 1,            'pressure': self.n_panels        }def create_dataloaders(    hdf5_file: str,    batch_size: int = 32,    train_split: float = 0.8,    val_split: float = 0.1,    shuffle: bool = True,    num_workers: int = 4,    device: str = 'cpu') -> Tuple[DataLoader, DataLoader, DataLoader]:    """    Create train/val/test dataloaders.        Args:        hdf5_file: Path to HDF5 dataset        batch_size: Batch size        train_split: Training set fraction        val_split: Validation set fraction        shuffle: Shuffle training data        num_workers: Number of data loading workers        device: Device for tensors            Returns:        (train_loader, val_loader, test_loader)    """    # Load full dataset    full_dataset = F1AeroDataset(hdf5_file, normalize=True, device=device)        # Compute split sizes    n_total = len(full_dataset)    n_train = int(n_total * train_split)    n_val = int(n_total * val_split)    n_test = n_total - n_train - n_val        # Split dataset    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(        full_dataset,        [n_train, n_val, n_test],        generator=torch.Generator().manual_seed(42)    )        # Create dataloaders    train_loader = DataLoader(        train_dataset,        batch_size=batch_size,        shuffle=shuffle,        num_workers=num_workers,        pin_memory=(device == 'cuda')    )        val_loader = DataLoader(        val_dataset,        batch_size=batch_size,        shuffle=False,        num_workers=num_workers,        pin_memory=(device == 'cuda')    )        test_loader = DataLoader(        test_dataset,        batch_size=batch_size,        shuffle=False,        num_workers=num_workers,        pin_memory=(device == 'cuda')    )        logger.info(f"Dataloaders created: train={n_train}, val={n_val}, test={n_test}")        return train_loader, val_loader, test_loaderclass AugmentedF1Dataset(F1AeroDataset):    """    Augmented dataset with data augmentation.    """        def __init__(        self,        hdf5_file: str,        augmentation_prob: float = 0.5,        noise_std: float = 0.01,        **kwargs    ):        """        Initialize augmented dataset.                Args:            hdf5_file: Path to HDF5 file            augmentation_prob: Probability of applying augmentation            noise_std: Standard deviation of Gaussian noise            **kwargs: Additional arguments for F1AeroDataset        """        super().__init__(hdf5_file, **kwargs)                self.augmentation_prob = augmentation_prob        self.noise_std = noise_std                logger.info(f"Augmented dataset: prob={augmentation_prob}, noise={noise_std}")        def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:        """Get sample with augmentation"""        inputs, outputs = super().__getitem__(idx)                # Apply augmentation with probability        if torch.rand(1).item() < self.augmentation_prob:            # Add Gaussian noise to inputs            noise = torch.randn_like(inputs) * self.noise_std            inputs = inputs + noise                return inputs, outputsif __name__ == "__main__":    # Test dataloader    logging.basicConfig(level=logging.INFO)        print("F1 Aero DataLoader Test")    print("=" * 60)        # Note: Requires actual HDF5 file from generate_dataset.py    hdf5_file = "data/training-datasets/f1_aero_dataset_test.h5"        if Path(hdf5_file).exists():        print(f"\nLoading dataset: {hdf5_file}")                # Create dataset        dataset = F1AeroDataset(hdf5_file, normalize=True)                print(f"\nDataset Info:")        print(f"  Samples: {len(dataset)}")        print(f"  Input dim: {dataset.get_input_dim()}")        print(f"  Output dims: {dataset.get_output_dims()}")                # Test single sample        inputs, outputs = dataset[0]        print(f"\nSample 0:")        print(f"  Inputs shape: {inputs.shape}")        print(f"  CL: {outputs['cl'].item():.4f}")        print(f"  CD: {outputs['cd'].item():.4f}")        print(f"  Pressure shape: {outputs['pressure'].shape}")                # Create dataloaders        train_loader, val_loader, test_loader = create_dataloaders(            hdf5_file,            batch_size=16,            device='cpu'        )                print(f"\nDataLoaders:")        print(f"  Train batches: {len(train_loader)}")        print(f"  Val batches: {len(val_loader)}")        print(f"  Test batches: {len(test_loader)}")                # Test batch        for batch_inputs, batch_outputs in train_loader:            print(f"\nBatch:")            print(f"  Inputs: {batch_inputs.shape}")            print(f"  CL: {batch_outputs['cl'].shape}")            print(f"  Pressure: {batch_outputs['pressure'].shape}")            break                print("\nÔ£à DataLoader test passed!")    else:        print(f"\nÔÜá´©Å  Dataset file not found: {hdf5_file}")        print("   Run generate_dataset.py first to create dataset")